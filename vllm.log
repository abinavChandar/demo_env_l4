INFO 10-14 19:22:01 [__init__.py:216] Automatically detected platform cuda.
2025-10-14 19:22:02.853794: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-10-14 19:22:02.871009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760469722.892123    4394 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760469722.898571    4394 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760469722.915302    4394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760469722.915333    4394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760469722.915336    4394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760469722.915339    4394 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-10-14 19:22:02.920208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:22:10 [api_server.py:1839] vLLM API server version 0.11.0
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:22:10 [utils.py:233] non-default args: {'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'trust_remote_code': True, 'max_model_len': 32000, 'gpu_memory_utilization': 0.92, 'max_num_seqs': 128}
[1;36m(APIServer pid=4394)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:22:29 [model.py:547] Resolved architecture: LlamaForCausalLM
[1;36m(APIServer pid=4394)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:22:29 [model.py:1510] Using max model len 32000
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:22:31 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 10-14 19:22:38 [__init__.py:216] Automatically detected platform cuda.
2025-10-14 19:22:39.740833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1760469759.761362    4671 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1760469759.767701    4671 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1760469759.783040    4671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760469759.783069    4671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760469759.783072    4671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1760469759.783086    4671 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:45 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:45 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":256,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:47 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4671)[0;0m WARNING 10-14 19:22:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:48 [gpu_model_runner.py:2602] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:48 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:22:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:24:39 [weight_utils.py:413] Time spent downloading weights for meta-llama/Meta-Llama-3.1-8B-Instruct: 109.554302 seconds
[1;36m(EngineCore_DP0 pid=4671)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=4671)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.31s/it]
[1;36m(EngineCore_DP0 pid=4671)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.35s/it]
[1;36m(EngineCore_DP0 pid=4671)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.08it/s]
[1;36m(EngineCore_DP0 pid=4671)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:21<00:00,  7.73s/it]
[1;36m(EngineCore_DP0 pid=4671)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:21<00:00,  5.32s/it]
[1;36m(EngineCore_DP0 pid=4671)[0;0m 
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:00 [default_loader.py:267] Loading weights took 21.38 seconds
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:01 [gpu_model_runner.py:2653] Model loading took 14.9889 GiB and 132.108878 seconds
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:09 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/14f1bc06e9/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:09 [backends.py:559] Dynamo bytecode transform time: 7.71 s
[1;36m(EngineCore_DP0 pid=4671)[0;0m [rank0]:W1014 19:25:11.084000 4671 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:13 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:34 [backends.py:218] Compiling a graph for dynamic shape takes 24.19 s
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:49 [monitor.py:34] torch.compile takes 31.90 s in total
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:50 [gpu_worker.py:298] Available KV cache memory: 4.75 GiB
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:51 [kv_cache_utils.py:1087] GPU KV cache size: 38,944 tokens
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:51 [kv_cache_utils.py:1091] Maximum concurrency for 32,000 tokens per request: 1.22x
[1;36m(EngineCore_DP0 pid=4671)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 1/35 [00:00<00:03,  8.51it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 2/35 [00:00<00:03,  8.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–Š         | 3/35 [00:00<00:03,  8.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆâ–        | 4/35 [00:00<00:03,  8.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–        | 5/35 [00:00<00:03,  8.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 6/35 [00:00<00:03,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 7/35 [00:00<00:03,  8.89it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:03,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–Œ       | 9/35 [00:01<00:02,  8.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:01<00:02,  8.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [00:01<00:02,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:01<00:02,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [00:01<00:02,  9.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:01<00:02,  8.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 15/35 [00:01<00:02,  9.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:01<00:02,  9.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [00:01<00:01,  9.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:01<00:01,  9.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [00:02<00:01,  9.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [00:02<00:01,  9.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:02<00:01,  9.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [00:02<00:01,  9.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:02<00:01,  9.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:02<00:00,  9.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:02<00:00, 10.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 29/35 [00:03<00:00, 10.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [00:03<00:00, 10.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [00:03<00:00, 10.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:03<00:00,  9.35it/s]
[1;36m(EngineCore_DP0 pid=4671)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   5%|â–Œ         | 1/19 [00:00<00:02,  8.98it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆ         | 2/19 [00:00<00:01,  9.42it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 3/19 [00:00<00:01,  9.68it/s]Capturing CUDA graphs (decode, FULL):  21%|â–ˆâ–ˆ        | 4/19 [00:00<00:01,  9.76it/s]Capturing CUDA graphs (decode, FULL):  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:01,  9.85it/s]Capturing CUDA graphs (decode, FULL):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:00<00:01,  9.89it/s]Capturing CUDA graphs (decode, FULL):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:00<00:01,  9.90it/s]Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:01<00:00, 10.05it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:01<00:00, 10.17it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:01<00:00, 10.26it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:01<00:00, 10.50it/s]Capturing CUDA graphs (decode, FULL):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:01<00:00, 10.67it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 10.29it/s]
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:57 [gpu_model_runner.py:3480] Graph capturing finished in 7 secs, took 0.38 GiB
[1;36m(EngineCore_DP0 pid=4671)[0;0m INFO 10-14 19:25:57 [core.py:210] init engine (profile, create kv cache, warmup model) took 56.00 seconds
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:25:59 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 2434
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:25:59 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=4394)[0;0m WARNING 10-14 19:25:59 [model.py:1389] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:25:59 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:25:59 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:26:00 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=4394)[0;0m INFO:     Started server process [4394]
[1;36m(APIServer pid=4394)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=4394)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:27:59 [chat_utils.py:560] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33172 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:28:10 [loggers.py:127] Engine 000: Avg prompt throughput: 7199.3 tokens/s, Avg generation throughput: 239.5 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 36.7%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33318 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33122 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:28:20 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 227.0 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.2%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:28:30 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.4 tokens/s, Running: 6 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.5%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:28:40 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.7 tokens/s, Running: 3 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.7%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:28:50 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 36.4 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.2%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:29:00 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.6 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.9%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33132 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:33196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:29:10 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:29:20 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 83.3%
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:31:50 [loggers.py:127] Engine 000: Avg prompt throughput: 4167.8 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 63 reqs, Waiting: 27 reqs, GPU KV cache usage: 76.2%, Prefix cache hit rate: 64.2%
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:32:00 [loggers.py:127] Engine 000: Avg prompt throughput: 1190.4 tokens/s, Avg generation throughput: 542.1 tokens/s, Running: 66 reqs, Waiting: 24 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 57.8%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45618 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45684 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45724 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45488 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45544 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45498 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45536 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:32:10 [loggers.py:127] Engine 000: Avg prompt throughput: 134.4 tokens/s, Avg generation throughput: 464.8 tokens/s, Running: 47 reqs, Waiting: 19 reqs, GPU KV cache usage: 98.9%, Prefix cache hit rate: 54.0%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45380 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45406 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:32:20 [loggers.py:127] Engine 000: Avg prompt throughput: 1161.6 tokens/s, Avg generation throughput: 329.1 tokens/s, Running: 38 reqs, Waiting: 10 reqs, GPU KV cache usage: 100.0%, Prefix cache hit rate: 53.7%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:32:30 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 366.1 tokens/s, Running: 33 reqs, Waiting: 13 reqs, GPU KV cache usage: 98.6%, Prefix cache hit rate: 54.4%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45564 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45634 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:32:40 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 307.9 tokens/s, Running: 27 reqs, Waiting: 13 reqs, GPU KV cache usage: 89.4%, Prefix cache hit rate: 54.7%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45664 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45698 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45710 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45748 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45936 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46004 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:32:50 [loggers.py:127] Engine 000: Avg prompt throughput: 838.3 tokens/s, Avg generation throughput: 247.2 tokens/s, Running: 29 reqs, Waiting: 2 reqs, GPU KV cache usage: 98.7%, Prefix cache hit rate: 52.8%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:33:00 [loggers.py:127] Engine 000: Avg prompt throughput: 207.4 tokens/s, Avg generation throughput: 289.8 tokens/s, Running: 26 reqs, Waiting: 1 reqs, GPU KV cache usage: 99.1%, Prefix cache hit rate: 53.0%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45980 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:33:10 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 260.4 tokens/s, Running: 21 reqs, Waiting: 0 reqs, GPU KV cache usage: 90.2%, Prefix cache hit rate: 53.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45812 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45816 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45764 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46062 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:33:20 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 176.4 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 37.6%, Prefix cache hit rate: 53.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45790 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46022 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45920 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:45914 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:33:30 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 55.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.1%, Prefix cache hit rate: 53.3%
[1;36m(APIServer pid=4394)[0;0m INFO:     127.0.0.1:46076 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:33:40 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
[1;36m(APIServer pid=4394)[0;0m INFO 10-14 19:33:50 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.3%
